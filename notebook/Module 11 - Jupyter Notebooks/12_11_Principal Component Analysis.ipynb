{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "## Outine\n",
    "- Introduction to Principal Component Analysis (PCA)\n",
    "- Manually calculating PCA\n",
    "- PCA using sklearn\n",
    "- Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "## Introduction to Principal Component Analysis\n",
    "   \n",
    "Principal component analysis (PCA) is a mathematical procedure that transforms a number of (possibly) correlated variables into a (smaller) number of uncorrelated variables called principal components.\n",
    "\n",
    "PCA is a method used to reduce number of variables in your data by extracting important ones from a large pool. It reduces the dimensionality of your data with the aim of retaining as much information as possible.\n",
    "\n",
    "To interpret each principal component, examine the magnitude and direction of the coefficients for the original variables. The larger the absolute value of the coefficient, the more important the corresponding variable is in calculating the component. How large the absolute value of a coefficient has to be in order to deem it important is subjective. Use your specialized knowledge to determine at what level the correlation value is important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Manually calculating PCA\n",
    "\n",
    "There is no **pca()** function in NumPy, but we can easily calculate the ``Principal Component Analysis`` step-by-step using NumPy functions.\n",
    "\n",
    "The example below defines a small 3×2 matrix, centers the data in the matrix, calculates the covariance matrix of the centered data, and then the eigendecomposition of the covariance matrix. The eigenvectors and eigenvalues are taken as the principal components and singular values and used to project the original data.\n",
    "\n",
    "PCA is an operation applied to a dataset, represented by an n x m matrix A that results in a projection of A which we will call B. Let’s walk through the steps of this operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "Calculate the means of each columns\n",
      "[3. 4.]\n",
      "Center columns\n",
      "[[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "Calculate covariance matrix of centered matrix\n",
      "[[4. 4.]\n",
      " [4. 4.]]\n",
      "eigenvectors (PCA components)\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "eigenvalues (PCA variance)\n",
      "[8. 0.]\n",
      "PCA reduction\n",
      "[[-2.82842712  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.82842712  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "# define a matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(\"Original matrix\")\n",
    "print(A)\n",
    "# step 1: calculate the mean of each column\n",
    "M = mean(A.T, axis=1)\n",
    "print(\"Calculate the means of each columns\")\n",
    "print(M)\n",
    "# step 2: center columns by subtracting column means\n",
    "C = A - M\n",
    "print(\"Center columns\")\n",
    "print(C)\n",
    "# step 3: calculate covariance matrix of centered matrix\n",
    "V = cov(C.T)\n",
    "print(\"Calculate covariance matrix of centered matrix\")\n",
    "print(V)\n",
    "# step 4: perform the eigendecomposition of covariance matrix\n",
    "values, vectors = eig(V)\n",
    "print(\"eigenvectors (PCA components)\")\n",
    "print(vectors)\n",
    "print(\"eigenvalues (PCA variance)\")\n",
    "print(values)\n",
    "# project data into the subspace (reduction)\n",
    "P = vectors.T.dot(C.T)\n",
    "print(\"PCA reduction\")\n",
    "print(P.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[1 3 5]\n",
      " [2 4 6]]\n",
      "Calculate the means of each columns\n",
      "[3. 4.]\n",
      "[[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "[[4. 4.]\n",
      " [4. 4.]]\n",
      "eigenvectors (PCA components)\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "eigenvalues (PCA variance)\n",
      "[8. 0.]\n",
      "PCA reduction\n",
      "[[-2.82842712  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.82842712  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "# define a matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(\"Original matrix\")\n",
    "print(A)\n",
    "print(A.T)\n",
    "# # step 1: calculate the mean of each column\n",
    "M = mean(A.T,axis=1)\n",
    "print(\"Calculate the means of each columns\")\n",
    "print(M)\n",
    "# # step 2: center columns by subtracting column means\n",
    "C = A - M\n",
    "# print(\"Center columns\")\n",
    "print(C)\n",
    "# step 3: calculate covariance matrix of centered matrix\n",
    "V = cov(C.T)\n",
    "# print(\"Calculate covariance matrix of centered matrix\")\n",
    "print(V)\n",
    "# # step 4: perform the eigendecomposition of covariance matrix\n",
    "values, vectors = eig(V)\n",
    "print(\"eigenvectors (PCA components)\")\n",
    "print(vectors)\n",
    "print(\"eigenvalues (PCA variance)\")\n",
    "print(values)\n",
    "# # project data into the subspace (reduction)\n",
    "P = vectors.T.dot(C.T)\n",
    "print(\"PCA reduction\")\n",
    "print(P.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA using sklearn\n",
    "\n",
    "We can calculate a Principal Component Analysis on a dataset using the PCA() class in the scikit-learn library. The benefit of this approach is that once the projection is calculated, it can be applied to new data again and again quite easily.\n",
    "\n",
    "When creating the class, the number of components can be specified as a parameter.\n",
    "\n",
    "The class is first fit on a dataset by calling the fit() function, and then the original dataset or other data can be projected into a subspace with the chosen number of dimensions by calling the transform() function.\n",
    "\n",
    "Once fit, the eigenvalues and principal components can be accessed on the PCA class via the explained_variance_ and components_ attributes.\n",
    "\n",
    "The example below demonstrates using this class by first creating an instance, fitting it on a 3×10 matrix, accessing the values and vectors of the projection, and transforming the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original matrix\n",
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27 28 29 30]]\n",
      "PCA components\n",
      "[[-0.31622777 -0.31622777 -0.31622777 -0.31622777 -0.31622777 -0.31622777\n",
      "  -0.31622777 -0.31622777 -0.31622777 -0.31622777]\n",
      " [ 0.9486833  -0.10540926 -0.10540926 -0.10540926 -0.10540926 -0.10540926\n",
      "  -0.10540926 -0.10540926 -0.10540926 -0.10540926]]\n",
      "PCA explained variance\n",
      "[1.00000000e+03 7.09974815e-30]\n",
      "PCA transform\n",
      "[[ 3.16227766e+01 -3.10862447e-15]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [-3.16227766e+01  3.10862447e-15]]\n"
     ]
    }
   ],
   "source": [
    "# Principal Component Analysis\n",
    "from numpy import array\n",
    "from sklearn.decomposition import PCA\n",
    "# define a matrix\n",
    "#A = array([[1, 2], [3, 4], [5, 6]])\n",
    "A = array([\n",
    "[1,2,3,4,5,6,7,8,9,10],\n",
    "[11,12,13,14,15,16,17,18,19,20],\n",
    "[21,22,23,24,25,26,27,28,29,30]])\n",
    "print('original matrix')\n",
    "print(A)\n",
    "# create the PCA instance\n",
    "pca = PCA(2)\n",
    "# fit on data\n",
    "pca.fit(A)\n",
    "# access values and vectors\n",
    "print('PCA components')\n",
    "print(pca.components_)\n",
    "print('PCA explained variance')\n",
    "print(pca.explained_variance_)\n",
    "# transform data\n",
    "B = pca.transform(A)\n",
    "print('PCA transform')\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that with some very minor floating point rounding that we achieve the same principal components, singular values, and projection as in the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "We discussed:\n",
    "- Introduction to Principal Component Analysis (PCA)\n",
    "- Manually calculating PCA\n",
    "- PCA using sklearn\n",
    "\n",
    "Examples, thanks to Jason Brownlee PhD retrieved from: https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
