{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "## Outine\n",
    "- Introduction to Principal Component Analysis (PCA)\n",
    "- Manually calculating PCA\n",
    "- PCA using sklearn\n",
    "- Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "## Dimensionality Reduction using Principal Component Analysis\n",
    "   \n",
    "Principal component analysis (PCA) is a mathematical procedure that transforms a number of (possibly) correlated variables into a (smaller) number of uncorrelated variables called principal components.\n",
    "\n",
    "PCA is a method used to reduce number of variables in your data by extracting important ones from a large pool. It reduces the dimensionality of your data with the aim of retaining as much information as possible.\n",
    "\n",
    "To interpret each principal component, examine the magnitude and direction of the coefficients for the original variables. The larger the absolute value of the coefficient, the more important the corresponding variable is in calculating the component. How large the absolute value of a coefficient has to be in order to deem it important is subjective. Use your specialized knowledge to determine at what level the correlation value is important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "## Eigenvalue and Eigenvectors\n",
    "   \n",
    "Eigenvectors are unit vectors, which means that their length or magnitude is equal to 1.0. They are often referred as right vectors, which simply means a column vector (as opposed to a row vector or a left vector). \n",
    "\n",
    "Eigenvalues are coefficients applied to eigenvectors that give the vectors their length or magnitude. For example, a negative eigenvalue may reverse the direction of the eigenvector as part of scaling it.\n",
    "\n",
    "A matrix that has only positive eigenvalues is referred to as a positive definite matrix, whereas if the eigenvalues are all negative, it is referred to as a negative definite matrix.\n",
    "\n",
    "For a square matrix A, an Eigenvector and Eigenvalue make this equation true (if we can find them, that is, it is solvable):\n",
    "\n",
    "<img src=\"images/eigenvalue.svg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Calculation of Eigendecomposition\n",
    "\n",
    "The example below first defines a 3×3 square matrix. The eigendecomposition is calculated on the matrix returning the eigenvalues and eigenvectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square matrix\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Eigenvalues\n",
      "[ 1.61168440e+01 -1.11684397e+00 -1.30367773e-15]\n",
      "Eigenvectors\n",
      "[[-0.23197069 -0.78583024  0.40824829]\n",
      " [-0.52532209 -0.08675134 -0.81649658]\n",
      " [-0.8186735   0.61232756  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "# eigendecomposition\n",
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"Square matrix\")\n",
    "print(A)\n",
    "# calculate eigendecomposition\n",
    "values, vectors = eig(A)\n",
    "print(\"Eigenvalues\")\n",
    "print(values)\n",
    "print(\"Eigenvectors\")\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Manually calculating PCA\n",
    "\n",
    "There is no **pca()** function in NumPy, but we can easily calculate the ``Principal Component Analysis`` step-by-step using NumPy functions.\n",
    "\n",
    "The example below defines a small 3×2 matrix, centers the data in the matrix, calculates the covariance matrix of the centered data, and then the eigendecomposition of the covariance matrix. The eigenvectors and eigenvalues are taken as the principal components and singular values and used to project the original data.\n",
    "\n",
    "PCA is an operation applied to a dataset, represented by an n x m matrix A that results in a projection of A which we will call B. Let’s walk through the steps of this operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "Calculate the means of each columns\n",
      "[3. 4.]\n",
      "Center columns\n",
      "[[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "Calculate covariance matrix of centered matrix\n",
      "[[4. 4.]\n",
      " [4. 4.]]\n",
      "eigenvectors (PCA components)\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "eigenvalues (PCA variance)\n",
      "[8. 0.]\n",
      "PCA reduction\n",
      "[[-2.82842712  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.82842712  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "# define a matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(\"Original matrix\")\n",
    "print(A)\n",
    "# step 1: calculate the mean of each column\n",
    "M = mean(A.T, axis=1)\n",
    "print(\"Calculate the means of each columns\")\n",
    "print(M)\n",
    "# step 2: center columns by subtracting column means\n",
    "C = A - M\n",
    "print(\"Center columns\")\n",
    "print(C)\n",
    "# step 3: calculate covariance matrix of centered matrix\n",
    "V = cov(C.T)\n",
    "print(\"Calculate covariance matrix of centered matrix\")\n",
    "print(V)\n",
    "# step 4: perform the eigendecomposition of covariance matrix\n",
    "values, vectors = eig(V)\n",
    "print(\"eigenvectors (PCA components)\")\n",
    "print(vectors)\n",
    "print(\"eigenvalues (PCA variance)\")\n",
    "print(values)\n",
    "# project data into the subspace (reduction)\n",
    "P = vectors.T.dot(C.T)\n",
    "print(\"PCA reduction\")\n",
    "print(P.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA using sklearn\n",
    "\n",
    "We can calculate a Principal Component Analysis on a dataset using the PCA() class in the scikit-learn library. The benefit of this approach is that once the projection is calculated, it can be applied to new data again and again quite easily.\n",
    "\n",
    "When creating the class, the number of components can be specified as a parameter.\n",
    "\n",
    "The class is first fit on a dataset by calling the fit() function, and then the original dataset or other data can be projected into a subspace with the chosen number of dimensions by calling the transform() function.\n",
    "\n",
    "Once fit, the eigenvalues and principal components can be accessed on the PCA class via the explained_variance_ and components_ attributes.\n",
    "\n",
    "The example below demonstrates using this class by first creating an instance, fitting it on a 3×10 matrix, accessing the values and vectors of the projection, and transforming the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original matrix\n",
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27 28 29 30]]\n",
      "PCA components\n",
      "[[-0.31622777 -0.31622777 -0.31622777 -0.31622777 -0.31622777 -0.31622777\n",
      "  -0.31622777 -0.31622777 -0.31622777 -0.31622777]\n",
      " [ 0.9486833  -0.10540926 -0.10540926 -0.10540926 -0.10540926 -0.10540926\n",
      "  -0.10540926 -0.10540926 -0.10540926 -0.10540926]]\n",
      "PCA explained variance\n",
      "[1.00000000e+03 7.09974815e-30]\n",
      "PCA transform\n",
      "[[ 3.16227766e+01 -3.10862447e-15]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [-3.16227766e+01  3.10862447e-15]]\n"
     ]
    }
   ],
   "source": [
    "# Principal Component Analysis\n",
    "from numpy import array\n",
    "from sklearn.decomposition import PCA\n",
    "# define a matrix\n",
    "#A = array([[1, 2], [3, 4], [5, 6]])\n",
    "A = array([\n",
    "[1,2,3,4,5,6,7,8,9,10],\n",
    "[11,12,13,14,15,16,17,18,19,20],\n",
    "[21,22,23,24,25,26,27,28,29,30]])\n",
    "print('original matrix')\n",
    "print(A)\n",
    "# create the PCA instance\n",
    "pca = PCA(2)\n",
    "# fit on data\n",
    "pca.fit(A)\n",
    "# access values and vectors\n",
    "print('PCA components')\n",
    "print(pca.components_)\n",
    "print('PCA explained variance')\n",
    "print(pca.explained_variance_)\n",
    "# transform data\n",
    "B = pca.transform(A)\n",
    "print('PCA transform')\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that with some very minor floating point rounding that we achieve the same principal components, singular values, and projection as in the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Introduction to Principal Component Analysis (PCA)\n",
    "- Manually calculating PCA\n",
    "- PCA using sklearn\n",
    "\n",
    "Examples, thanks to Jason Brownlee PhD retrieved from: https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit: Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "### Dimensionality Reduction using Singular Value Decomposition (SVD)\n",
    "   \n",
    "SVD means Singular Value Decomposition. The SVD of a matrix X of dimension n×d is given by:\n",
    "\n",
    "**X = UDV<sup>T</sup>**\n",
    "\n",
    "Where U and V are square orthogonal:\n",
    "\n",
    "**U<sup>T</sup>U = I<sub>d</sub>**\n",
    "\n",
    "**V<sup>T</sup>V = I<sub>d</sub>**\n",
    "\n",
    "and D is diagonal of dimension d x n\n",
    "\n",
    "Some additional notes:\n",
    "- D is not necessarily square\n",
    "- The SVD of a matrix can be done for any matrix\n",
    "- SVD is different from the eigenvalue decomposition of a matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Calculate SVD\n",
    "\n",
    "The SVD can be calculated by calling the **svd()** function.\n",
    "\n",
    "The function takes a matrix and returns the U, D and V<sup>T</sup> elements. The **D** diagonal matrix is returned as a vector of singular values. The V matrix is returned in a transposed form, e.g. V<sup>T</sup>\n",
    "\n",
    "The example below defines a 3×2 matrix and calculates the Singular-value decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original matrix (3, 2)\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "U matrix\n",
      "[[-0.2298477   0.88346102  0.40824829]\n",
      " [-0.52474482  0.24078249 -0.81649658]\n",
      " [-0.81964194 -0.40189603  0.40824829]]\n",
      "D matrix\n",
      "[9.52551809 0.51430058]\n",
      "V Transpose matrix\n",
      "[[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n"
     ]
    }
   ],
   "source": [
    "# Singular-value decomposition\n",
    "from numpy import array\n",
    "from scipy.linalg import svd\n",
    "# define a matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(\"original matrix\", A.shape)\n",
    "print(A)\n",
    "# SVD\n",
    "U, D, VT = svd(A)\n",
    "print(\"U matrix\")\n",
    "print(U)\n",
    "print(\"D matrix\")\n",
    "print(D)\n",
    "print(\"V Transpose matrix\")\n",
    "print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of SVD for pseudoinverse\n",
    "The pseudoinverse is the generalization of the matrix inverse for square matrices to rectangular matrices where the number of rows and columns are not equal.\n",
    "\n",
    "The pseudoinverse provides one way of solving the linear regression equation, specifically when there are more rows than there are columns, which is often the case.\n",
    "\n",
    "NumPy provides the function pinv() for calculating the pseudoinverse of a rectangular matrix.\n",
    "\n",
    "The example below defines a 4×2 matrix and calculates the pseudoinverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original matrix\n",
      "[[0.1 0.2]\n",
      " [0.3 0.4]\n",
      " [0.5 0.6]\n",
      " [0.7 0.8]]\n",
      "pseudoinverse matrix\n",
      "[[-1.00000000e+01 -5.00000000e+00  1.28757642e-14  5.00000000e+00]\n",
      " [ 8.50000000e+00  4.50000000e+00  5.00000000e-01 -3.50000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Pseudoinverse\n",
    "from numpy import array\n",
    "from numpy.linalg import pinv\n",
    "# define matrix\n",
    "A = array([\n",
    "[0.1, 0.2],\n",
    "[0.3, 0.4],\n",
    "[0.5, 0.6],\n",
    "[0.7, 0.8]])\n",
    "print(\"original matrix\")\n",
    "print(A)\n",
    "# calculate pseudoinverse\n",
    "B = pinv(A)\n",
    "print(\"pseudoinverse matrix\")\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of SVD for Dimensionality Reduction\n",
    "A popular application of SVD is for dimensionality reduction.\n",
    "\n",
    "Data with a large number of features, such as more features (columns) than observations (rows) may be reduced to a smaller subset of features that are most relevant to the prediction problem.\n",
    "\n",
    "The result is a matrix with a lower rank that is said to approximate the original matrix.\n",
    "\n",
    "To do this we can perform an SVD operation on the original data and select the top k largest singular values in the Diagonal. These columns can be selected from the Diagonal and the rows selected from V<sup>T</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original matrix\n",
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27 28 29 30]]\n",
      "Reconstructed matrix\n",
      "[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [11. 12. 13. 14. 15. 16. 17. 18. 19. 20.]\n",
      " [21. 22. 23. 24. 25. 26. 27. 28. 29. 30.]]\n",
      "T matrix (reduced dimensionality), U dot product with Digaonal\n",
      "[[-18.52157747   6.47697214]\n",
      " [-49.81310011   1.91182038]\n",
      " [-81.10462276  -2.65333138]]\n",
      "T matrix (reduced dimensionality), A dot product with VT.T\n",
      "[[-18.52157747   6.47697214]\n",
      " [-49.81310011   1.91182038]\n",
      " [-81.10462276  -2.65333138]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "# define a matrix\n",
    "A = array([\n",
    "[1,2,3,4,5,6,7,8,9,10],\n",
    "[11,12,13,14,15,16,17,18,19,20],\n",
    "[21,22,23,24,25,26,27,28,29,30]])\n",
    "print(\"original matrix\")\n",
    "print(A)\n",
    "# Singular-value decomposition\n",
    "U, D, VT = svd(A)\n",
    "# create m x n Diagonal matrix\n",
    "Diagonal = zeros((A.shape[0], A.shape[1]))\n",
    "# populate Sigma with n x n diagonal matrix\n",
    "Diagonal[:A.shape[0], :A.shape[0]] = diag(D)\n",
    "# select the number of elements\n",
    "n_elements = 2\n",
    "Diagonal = Diagonal[:, :n_elements]\n",
    "VT = VT[:n_elements, :]\n",
    "# reconstruct\n",
    "B = U.dot(Diagonal.dot(VT))\n",
    "print(\"Reconstructed matrix\")\n",
    "print(B)\n",
    "# transform\n",
    "T = U.dot(Diagonal)\n",
    "print(\"T matrix (reduced dimensionality), U dot product with Digaonal\")\n",
    "print(T)\n",
    "T = A.dot(VT.T)\n",
    "print(\"T matrix (reduced dimensionality), A dot product with VT.T\")\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples, thanks to Jason Brownlee PhD retrieved from: https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
